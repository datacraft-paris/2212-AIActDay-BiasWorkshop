{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7546d2ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](../../images/datacraft-logo.png)\n",
    "\n",
    "<h1><center>AI Act Day</center></h1>\n",
    "<center><i>Datacraft workshop</i></center>\n",
    "\n",
    "\n",
    "**Objective of the notebook**: understand how to tackle the fairness challenge in AI on real use case - HR recrutment\n",
    "\n",
    "This exercise highly relies on the work conducted by the Dalex team (see [Dalex documentation](https://dalex.drwhy.ai/), [Fairness module in Dalex](https://dalex.drwhy.ai/python-dalex-fairness.html) or [Advanced tutorial on bias detection in Dalex](https://dalex.drwhy.ai/python-dalex-fairness2.html)). It is based on two [Stackoverflow survey](https://insights.stackoverflow.com/survey) (2021 & 2022) adapted to a recrutment issue.\n",
    "\n",
    "Our humble ambition is to combine the power of different tools: Dalex as a base, complemented by other tools from the open source community, like [AIF360](https://aif360.mybluemix.net/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3664e9",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prerequesites\" data-toc-modified-id=\"Prerequesites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prerequesites</a></span></li><li><span><a href=\"#Bias-a-priori\" data-toc-modified-id=\"Bias-a-priori-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bias a priori</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>EDA</a></span></li><li><span><a href=\"#ML-prerequisites\" data-toc-modified-id=\"ML-prerequisites-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>ML prerequisites</a></span></li><li><span><a href=\"#Fairness-evaluation-principles\" data-toc-modified-id=\"Fairness-evaluation-principles-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Fairness evaluation principles</a></span></li><li><span><a href=\"#Strategies\" data-toc-modified-id=\"Strategies-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Strategies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Do-nothing\" data-toc-modified-id=\"Do-nothing-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Do nothing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Algorithmic-performance\" data-toc-modified-id=\"Algorithmic-performance-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Algorithmic performance</a></span></li><li><span><a href=\"#Fairness-performance\" data-toc-modified-id=\"Fairness-performance-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>Fairness performance</a></span></li></ul></li><li><span><a href=\"#Remove-sensitive-attribute\" data-toc-modified-id=\"Remove-sensitive-attribute-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Remove sensitive attribute</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Algorithmic-performance\" data-toc-modified-id=\"Algorithmic-performance-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Algorithmic performance</a></span></li><li><span><a href=\"#Fairness-performance\" data-toc-modified-id=\"Fairness-performance-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>Fairness performance</a></span></li></ul></li><li><span><a href=\"#Adversarial-inprocessing\" data-toc-modified-id=\"Adversarial-inprocessing-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Adversarial inprocessing</a></span></li><li><span><a href=\"#Calibrate-equalized-ODTS\" data-toc-modified-id=\"Calibrate-equalized-ODTS-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Calibrate equalized ODTS</a></span></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Comparison</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82455bd4",
   "metadata": {},
   "source": [
    "## Prerequesites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165a31d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:53:50.393927Z",
     "start_time": "2021-11-08T16:53:50.384927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We strongly recommend to install the necessary libraries from a dedicated virtual environment\n",
    "# See Minicoda e.g.: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands\n",
    "\n",
    "# ! pip install fairlearn\n",
    "# ! pip install dalex -U\n",
    "# ! pip install -U scikit-learn\n",
    "# ! pip install -U pandas\n",
    "# ! pip install - U aif360 \n",
    "# ! pip install -U plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecff085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:53:59.159825Z",
     "start_time": "2021-11-08T16:53:50.592927Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import sklearn\n",
    "import dalex as dx\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "{\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"matplotlib\": matplotlib.__version__,\n",
    "    \"seaborn\": sns.__version__,\n",
    "    \"sklearn\": sklearn.__version__,\n",
    "    \"dalex\": dx.__version__,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65464feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ddb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:53:59.174788Z",
     "start_time": "2021-11-08T16:53:59.161789Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23066dcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Bias a priori\n",
    "\n",
    "*When implementing an AI system, fairness and biases must be an important component during conception, especially when dealing with sensitive information, and/or Personally Identifiable Information (PII), and/or Personal Health Information (PHI). Indeed, not only those information are bound to the law (GDPR in Europe e.g.), but they are also bound to a brand image challenge.*\n",
    "\n",
    "Today's example aims at **assigning a risk with recruitment data**.\n",
    "\n",
    "Before implementing any AI system to predict the likelihood of a candidate to be hired, **AI engineers AND business stakeholders** should:\n",
    "\n",
    "- Sit and identify potential sources of biases\n",
    "- Define one or several metrics that will quantify the bias of the AI system\n",
    "\n",
    "![fairness_tree](../../images/fairness_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25809c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "StackOverflow's annual user-generated survey (over 70,000 responses from over 180 countries) of developers examines all aspects of the developer experience, from learning code to preferred technologies, version control and work experience. From the survey results, we have built a dataset with the following columns.\n",
    "\n",
    "The columns of the dataset are :\n",
    "- **Age**: age of the applicant, >35 years old or <35 years old *(categorical)*\n",
    "- **EdLevel**: education level of the applicant (Undergraduate, Master, PhD...) *(categorical)*\n",
    "- **Gender**: gender of the applicant, (Man, Woman, or NonBinary) *(categorical)*\n",
    "- **MainBranch**: whether the applicant is a profesional developer *(categorical)*\n",
    "- **YearsCode**: how long the applicant has been coding *(integer)*\n",
    "- **YearsCodePro**: how long the applicant has been coding in a professional context, *(integer)*\n",
    "- **PreviousSalary**: the applicant's previous job salary *(float)*\n",
    "- **ComputerSkills**: number of computer skills known by the applicant *(integer)*\n",
    "- **Employed**: target variable, whether the applicant has been hired *(categorical)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2b9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:01.904204Z",
     "start_time": "2021-11-08T16:54:01.880199Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('stackoverflow.csv', index_col=0)\n",
    "target = \"Employed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4e33f-71dc-45e6-bc9a-892c706b6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "df.sample(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222225d-ca49-421f-bbd4-e28a9c633167",
   "metadata": {},
   "source": [
    "A priori, the sensitive variables from a recruitment perspective are Age and Gender. We first investigate this intuition with an exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4d631",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "#### Visualization of data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195132fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.Blues  # each subplot has its own blue color\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (12,8))\n",
    "\n",
    "# Age\n",
    "_ = df.Age.value_counts().plot.bar(subplots=True,\n",
    "                                   ax=ax[0, 0],\n",
    "                                   rot=1,\n",
    "                                   color=cmap(0.2))\n",
    "# Gender\n",
    "_ = df.Gender.value_counts().plot.bar(subplots=True,\n",
    "                                      ax=ax[0, 1],\n",
    "                                      rot=1,\n",
    "                                      color=cmap(0.3))\n",
    "# MainBranch\n",
    "_ = df.MainBranch.value_counts().plot.bar(subplots=True,\n",
    "                                          ax=ax[0, 2],\n",
    "                                          rot=1,\n",
    "                                          color=cmap(0.4))\n",
    "# EdLevel\n",
    "_ = df.EdLevel.value_counts().plot.barh(subplots=True,\n",
    "                                        ax=ax[1, 0],\n",
    "                                        color=cmap(0.5))\n",
    "\n",
    "# Employed\n",
    "_ = df.Employed.value_counts().plot.bar(subplots=True,\n",
    "                                        ax=ax[1, 1],\n",
    "                                        rot=1,\n",
    "                                        color=cmap(0.6))\n",
    "\n",
    "# ComputerSkills\n",
    "_ = df.ComputerSkills.clip(None, 30).plot.hist(subplots=True,\n",
    "                                               ax= ax[1, 2],\n",
    "                                               rot=1,\n",
    "                                               color=cmap(0.7), \n",
    "                                               alpha=0.9, \n",
    "                                               edgecolor='w')\n",
    "\n",
    "ax[1, 2].title.set_text('ComputerSkills')\n",
    "\n",
    "# YearsCode\n",
    "_ = df.YearsCode.plot.hist(subplots=True,\n",
    "                                ax= ax[2, 0],\n",
    "                                rot=1,\n",
    "                                color=cmap(0.8),\n",
    "                                alpha=0.9, \n",
    "                                edgecolor='w')\n",
    "\n",
    "ax[2, 0].title.set_text('YearsCode')\n",
    "\n",
    "# YearsCodePro\n",
    "_ = df.YearsCodePro.plot.hist(subplots=True,\n",
    "                              ax= ax[2, 1],\n",
    "                              rot=1,\n",
    "                              color=cmap(0.9),\n",
    "                              alpha=0.9, \n",
    "                              edgecolor='w')\n",
    "\n",
    "ax[2, 1].title.set_text('YearsCodePro')\n",
    "\n",
    "# PreviousSalary\n",
    "_ = df.PreviousSalary.plot.hist(subplots=True,\n",
    "                                ax= ax[2, 2],\n",
    "                                rot=1,\n",
    "                                color=cmap(1.0),\n",
    "                                alpha=0.9, \n",
    "                                edgecolor='w')\n",
    "\n",
    "ax[2, 2].title.set_text('PreviousSalary')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d695c",
   "metadata": {},
   "source": [
    "#### Visualization of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df[[\"ComputerSkills\", \"YearsCode\", \"YearsCodePro\"]], palette=[cmap(0.7), cmap(0.8), cmap(0.9)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df[\"PreviousSalary\"], palette=[cmap(1.0)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a072cc5",
   "metadata": {},
   "source": [
    "#### Visualization of gender bias on Employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f232136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:39:26.546261Z",
     "start_time": "2021-11-08T16:39:26.529227Z"
    },
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "var = (df\n",
    " .groupby(\"Gender\")[target]\n",
    " .value_counts(normalize=True)\n",
    " .multiply(100)\n",
    " .round(1)\n",
    " .rename(\"%\")\n",
    " .reset_index()\n",
    ")\n",
    "\n",
    "sns.barplot(data=var, ci=None, x=\"Gender\", y=\"%\", hue=\"Employed\", width=0.5, palette=[\"orangered\", \"forestgreen\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0ed5e",
   "metadata": {},
   "source": [
    "#### Visualization of age bias on Employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6e2ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:39:26.546261Z",
     "start_time": "2021-11-08T16:39:26.529227Z"
    },
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "var = (df\n",
    " .groupby(\"Age\")[target]\n",
    " .value_counts(normalize=True)\n",
    " .multiply(100)\n",
    " .round(1)\n",
    " .rename(\"%\")\n",
    " .reset_index()\n",
    ")\n",
    "\n",
    "sns.barplot(data=var, ci=None, x=\"Age\", y=\"%\", hue=\"Employed\", width=0.5, palette=[\"orangered\", \"forestgreen\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461871cf",
   "metadata": {},
   "source": [
    "#### Visualization of gender and age bias on Employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = (df\n",
    " .groupby([\"Gender\", \"Age\"])[target]\n",
    " .value_counts(normalize=True)\n",
    " .multiply(100)\n",
    " .round(1)\n",
    " .rename(\"%\")\n",
    " .reset_index()\n",
    ")\n",
    "\n",
    "sns.catplot(data=var,\n",
    "            kind=\"bar\",\n",
    "            col=\"Age\",\n",
    "            row=\"Gender\",\n",
    "            x=\"Employed\",\n",
    "            y=\"%\",\n",
    "            palette=[\"orangered\", \"forestgreen\"],\n",
    "            height=2,\n",
    "            aspect=1.5,\n",
    "            margin_titles=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e88e98",
   "metadata": {},
   "source": [
    "#### Analytics on biases for Gender and Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .groupby(\"Gender\")[target]\n",
    " .value_counts(normalize=True)\n",
    " .multiply(100)\n",
    " .round(1)\n",
    " .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b598dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .groupby(\"Age\")[target]\n",
    " .value_counts(normalize=True)\n",
    " .multiply(100)\n",
    " .round(1)\n",
    " .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"Gender\"], df[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .groupby([\"Gender\", \"Age\"])[target]\n",
    " .value_counts(normalize=True)\n",
    " .multiply(100)\n",
    " .round(1)\n",
    " .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e940a",
   "metadata": {},
   "source": [
    "**Quick EDA**\n",
    "\n",
    "If we bin by gender, we realize that women are:\n",
    "- Under represented in volume: 3,518 vs. 68,573   \n",
    "- Biased from an employment perspective: 44.9% vs. 54.1%\n",
    "\n",
    "If we bin by age, we realize that people over 35 are:\n",
    "- Under represented in volume: 25,643 vs. 47,819\n",
    "- Biased from an employment perspective: 51.6% vs. 54.7%\n",
    "\n",
    "\n",
    "If we combine the two, biases in input data are amplified:\n",
    "- Under represented in volume: 938 vs. 44,343\n",
    "- Biased from an employment perspective: 41.0% vs. 55.2%\n",
    "\n",
    "**Conclusion: the apriori looks confirmed and will need to be carefully handled during modelling.**\n",
    "\n",
    "----\n",
    "\n",
    "**Notes**\n",
    "\n",
    "1. This is a toy example where biases are \"straightforward\" and well identified as \"recurrent\" social biases. However, it might not always be as easy to detect them. Additional sources might come from data history, selection bias, data incompleteness, unexpected sources of bias (column/attribute), ...\n",
    "2. In this example, we identified biases related to representation in volume. If we had not been exposed to such discrepencies, namely having a balanced dataset, could we have concluded that biases would have been limited while modelling? Not so sure, see [this article](https://arxiv.org/pdf/1811.08489.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327ea81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ML prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9f683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:11.096850Z",
     "start_time": "2021-11-08T16:54:11.078844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=target),\n",
    "    df[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e90df",
   "metadata": {},
   "source": [
    "## Fairness evaluation principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6643757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T16:59:52.131656Z",
     "start_time": "2021-12-01T16:59:52.032435Z"
    }
   },
   "source": [
    "![dalex](../../images/dalex_pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77c1f3",
   "metadata": {},
   "source": [
    "The main object of dalex is the `Explainer` container which wraps a **dataset** (features and target) and a **trained model**. \n",
    "\n",
    "Once the data and the model have been wrapped, one needs to fix **protected and privileged attributes**.\n",
    "\n",
    "**Important note**: beware these choices correspond to an a priori understanding of the problem and could miss hidden flaws of the model. An interesting line of work would consist in conducting a kind of grid-search exploration for potential biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bbe92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:18.644263Z",
     "start_time": "2021-11-08T16:54:18.628267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Protected attribute is 0 if a man or non binary and 0 if a woman plus the age\n",
    "\n",
    "protected = (pd.Series(np.where(X_test[\"Gender\"] == \"Woman\", '1', '0'), index=X_test.index) \n",
    "             + '_' \n",
    "             + X_test.Age)\n",
    "protected_train = (pd.Series(np.where(X_train[\"Gender\"] == \"Woman\", '1', '0').astype(str), index=X_train.index) \n",
    "                   + '_' \n",
    "                   + X_train.Age)\n",
    "\n",
    "# Privileged population is men under 35 years old\n",
    "privileged = '0_<35'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736579d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Strategies\n",
    "\n",
    "Following section intends to implement different strategies to mitigate the bias:\n",
    "- No strategy implemented\n",
    "- Pre-processing strategy: edit the data priori to fitting a model\n",
    "- In-processing: change the way a model is trained, changing the loss function e.g.\n",
    "- Post-processing: edit the predictions once a model has been fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d156f12",
   "metadata": {},
   "source": [
    "### Do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc2820",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b286cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:26.651675Z",
     "start_time": "2021-11-08T16:54:26.635675Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "      (\"passthrough\", make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\", sparse=False), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "clf_decisiontree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=10, random_state=123))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b937a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:27.294932Z",
     "start_time": "2021-11-08T16:54:27.250331Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf_decisiontree.fit(df.drop(columns=[target]), df[target])\n",
    "clf_decisiontree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8889fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:56.205145Z",
     "start_time": "2021-11-08T16:54:56.177145Z"
    }
   },
   "outputs": [],
   "source": [
    "# exp_decisiontree = dx.Explainer(clf_decisiontree, df.drop(columns=[target]), df[target], verbose=False)\n",
    "exp_decisiontree = dx.Explainer(clf_decisiontree, X_test, y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259302a4",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33dff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:04.966640Z",
     "start_time": "2021-11-08T16:55:04.910641Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_decisiontree.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f2be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fairness performance\n",
    "\n",
    "Quoting Dalex' tutorial:\n",
    "\n",
    "\n",
    "> The idea is that ratios between scores of privileged and unprivileged metrics should be close to 1. The closer, the fairer. To relax this criterion a little bit, it can be written more thoughtfully:\n",
    "\n",
    "> $$ \\forall i \\in \\{a, b, ..., z\\}, \\quad \\epsilon < \\frac{metric_i}{metric_{privileged}} < \\frac{1}{\\epsilon}.$$\n",
    "\n",
    "> Where the epsilon is a value between 0 and 1, it should be a minimum acceptable value of the ratio. On default, it is 0.8, which adheres to four-fifths rule (80% rule) often looked at in hiring, for example.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccd794",
   "metadata": {},
   "source": [
    "#### Metrics used\n",
    "\n",
    "\n",
    "- **Equal opportunity ratio** computed from True positive rate (recall)\n",
    "\n",
    "> This number describes the proportions of correctly classified positive instances.\n",
    "\n",
    "> $TPR = \\frac{TP}{P}$\n",
    "\n",
    "- **Predictive parity ratio** computed from Positive predicted value (precision)\n",
    "\n",
    "> This number describes the ratio of samples which were correctly classified as positive from all the positive predictions.\n",
    "\n",
    "> $PPV = \\frac{TP}{TP + FP}$\n",
    "\n",
    "- **Accuracy equality ratio** computed from Accuracy\n",
    "\n",
    "> This number is the ratio of the correctly classified instances (positive and negative) of all decisions.\n",
    "\n",
    "> $ACC = \\frac{TP + TN}{TP + FP + TN + FN}$\n",
    "\n",
    "- **Predictive equality ratio** computed from False positive rate\n",
    "\n",
    "> This number describes the share the proportion of actual negatives which was falsely classified as positive.\n",
    "\n",
    "> $FPR = \\frac{FP}{TP + TN}$\n",
    "\n",
    "- **Statistical parity ratio** computed from Positive rate\n",
    "\n",
    "> This number is the overall rate of positively classified instances, including both correct and incorrect decisions.\n",
    "\n",
    "> $PR = \\frac{TP + FP}{TP + FP + TN + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aec8ac",
   "metadata": {},
   "source": [
    "The method `model_fairness` returns a fairness object from which fairness evaluations can be conducted. Notice that every metrics inherited from the confusion matrix are computed during the instantiation.\n",
    "\n",
    "Two methods can then be performed:\n",
    "- The `fairness_check` method, which returns a report on the fairness of the model. It requires an epsilon parameter that corresponds to the threshold ratio below which a given metric is considered to be unfair (default value is 0.8).\n",
    "- The `plot` method, which allows to visualize the main fairness ratios between the protected subgroups and the privileged one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18a463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:10.877525Z",
     "start_time": "2021-11-08T16:55:10.854526Z"
    }
   },
   "outputs": [],
   "source": [
    "fairness_decisiontree = exp_decisiontree.model_fairness(protected=protected, privileged=privileged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83c47a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:11.815330Z",
     "start_time": "2021-11-08T16:55:11.795325Z"
    }
   },
   "outputs": [],
   "source": [
    "fairness_decisiontree.fairness_check(epsilon = 0.8) # default epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caca6b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:15.708181Z",
     "start_time": "2021-11-08T16:55:13.726179Z"
    }
   },
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc40d8f",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "1. Fairness metrics work the exact same way as performance metrics do. If one was to fit a model on the entire dataset and foster overfitting (namely, skipping a `train_test_split` operation), she would end up with a non biased model.\n",
    "2. A lots of metrics can be computed. It is important to define early in the conception which are the critical metrics to monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc19112",
   "metadata": {},
   "source": [
    "### Pre-processing: Remove sensitive attribute\n",
    "\n",
    "The first thing that can come to mind is to remove sensitive variables. However, this option is considered as really naive since it can have no effect. \n",
    "\n",
    "**When may this method works ?**\n",
    "\n",
    "➤ If the sensitive variable conveys the bias (mostly) on its own.\n",
    "\n",
    "    ➤ This means, the sensitive variable is correlated with the target variable\n",
    "    ➤ This also means, the sensitive variable is NOT correlated at all with other explanatory variables and any combination of other explanatory variables cannot be used as a proxi for the sensitive variable. \n",
    "\n",
    "Most of the time, the bias is shared accross explanatory variables. For example, a bias on gender may be present in many ones (salary, education, socio-professional category, etc.)\n",
    "\n",
    "This transformation is not possible in the dalex module (since it not a recommended option to deal with bias). To implement it, a new model has to be trained and explained. This time the sensitive variable\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain a model without sensitive variables \"age\" and \"sex\"\n",
    "X_train_restricted = X_train.drop(['Gender', 'Age'], axis=1)\n",
    "\n",
    "preprocessor_restr = make_column_transformer(\n",
    "      (\"passthrough\", make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\", sparse=False), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "clf_decisiontree_restr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_restr),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "])\n",
    "\n",
    "clf_decisiontree_restr.fit(X_train_restricted, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215a2d9",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601caa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dalex explainer for the model without sensitive variables\n",
    "exp_decisiontree_restr = dx.Explainer(clf_decisiontree_restr, X_test, y_test, verbose=True)\n",
    "\n",
    "exp_decisiontree_restr.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34616f",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "1. Here are the performance metrics for the new model. Results are quite similar to those of the model with all variables. However, considering we want to unbias results, it's quite difficult to use these metrics to compare models. Indeed, having a 100% precision on predicting a bias decision is far from our goal event if the model is perfect (at its predicting job).\n",
    "\n",
    "#### Fairness performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54298d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see effect of removing sensitive variables on bias metrics\n",
    "fairness_decisiontree_restr = exp_decisiontree_restr.model_fairness(protected=protected, privileged=privileged, \n",
    "                                                                    label='DecisionTreeClassifier_no_sensitive')\n",
    "\n",
    "fairness_decisiontree_restr.fairness_check(epsilon = 0.8) # default epsilon\n",
    "\n",
    "fairness_decisiontree_restr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57210c",
   "metadata": {},
   "source": [
    "Without a real surprise, the effect of removing sensitive variables did not unbias our results. In dalex it's also possible to compare models performances (according to bias metrics). Below, there is the comparison between the decision tree with and without sensitive variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2 (or more) fairness objects in dalex (add them as list in parameters). \n",
    "# It's also possible to choose the plot type ! \n",
    "fairness_decisiontree_restr.plot([fairness_decisiontree], type='radar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288399a3-1bdf-4282-a662-0fcfd11fb013",
   "metadata": {},
   "source": [
    "### [Optional] Explaining the classifier's decisions\n",
    "\n",
    "Before looking into bias mitigation techniques, we can try to interpret the classifier's predictions with explainability techniques. In our case, we have used a [decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) and can therefore look at feature importances, or even plot the full tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5b2f0-6c36-4291-b3e1-df1ec35d0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf_decisiontree['classifier'].feature_importances_\n",
    "\n",
    "decisiontree_importances = pd.Series(importances, index = clf_decisiontree['preprocessor'].get_feature_names())\n",
    "\n",
    "decisiontree_importances.plot.barh(title = 'Feature importances in Decisiontree',\n",
    "                                   color = 'lightseagreen')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce1978-7fa2-4ee9-9437-c633011b4198",
   "metadata": {},
   "source": [
    "Despite the biases observed previously, the age and gender variables do not seem important for the decision tree's predictions. The most import feature is the number of skills of the applicant. One explanation for the biases may be that the number of skills is correlated to age or gender, which we investigate in the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d11414-0ff2-48f1-bcbf-0b329994a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,\n",
    "            kind=\"box\",\n",
    "            x=\"Age\",\n",
    "            y=\"ComputerSkills\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b386c6d-9349-4b91-83e4-be70523c04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,\n",
    "            kind=\"box\",\n",
    "            x=\"Gender\",\n",
    "            y=\"ComputerSkills\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7295d5c",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Train a decision tree with a small depth, which allows toplot the entire tree and have an explainable model. Can you see some biases in the tree's decision path?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655120e8",
   "metadata": {},
   "source": [
    "Now, let's check more appropriate ways to remove bias\n",
    "\n",
    "### Pre-processing: Resampling\n",
    "\n",
    "Dalex provide 2 types of resampling methods and 1 reweighting method. In this tutorial only the basic resampling is showed.\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalex.fairness import resample\n",
    "clf_resampled = copy(clf_decisiontree) # Create a copy to not alter the main object\n",
    "\n",
    "# Resampling observations\n",
    "indices_uniform = resample(protected_train, y_train, verbose = False)\n",
    "\n",
    "# Re-fit model with resampled data\n",
    "clf_resampled.fit(X_train.reset_index(drop=True).iloc[indices_uniform, :], y_train.reset_index(drop=True)[indices_uniform])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59593803",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decisiontree_resampled = dx.Explainer(clf_resampled, X_test, y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d3d3d",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3094f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decisiontree_resampled.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75950a",
   "metadata": {},
   "source": [
    "#### Fairness performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree_resampled = exp_decisiontree_resampled.model_fairness(\n",
    "    protected, privileged, label='DecisionTreeClassifier_resampled')\n",
    "\n",
    "fairness_decisiontree_resampled.fairness_check(epsilon = 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e469789",
   "metadata": {},
   "source": [
    "__Compare performance of the first model and the resampled one (visually)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot([fairness_decisiontree_resampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9be4ee",
   "metadata": {},
   "source": [
    "The resampling method is partly random but it should increase the fairness of outputs at least on few fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f237c-705d-4695-a498-2548f6e46fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot([fairness_decisiontree_resampled, fairness_decisiontree_restr], type='radar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b40225-7b34-4beb-b11c-68bbe4e30570",
   "metadata": {},
   "source": [
    "This resampling method seems to have succeeded at removing biases for our chosen threhsold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4074164",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In-processing: Adversarial training\n",
    "\n",
    "The adversarial inprocessing method consists in learning a target attribute $y$ (here the risk) while forgetting a fixed sensitive attribute $s$. This is done by learning a neural network and minimizing a loss of the form:\n",
    "$$ \\mathcal{L} = \\mathcal{L}_{CE}(y,\\hat{y}) - \\lambda \\mathcal{L}_{CE}(s,\\hat{s}), $$\n",
    "where $\\lambda$ controls the fairness-accuracy tradeoff. \n",
    "\n",
    "This method is implemented in aif360 in the case of a binary sensitive attribute. In the following we incorporate it into the dalex pipeline.\n",
    "\n",
    "For simplicity, we only investigate biases with respect to the variable Gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.sklearn.inprocessing import AdversarialDebiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "sess = tf.Session()\n",
    "tf.disable_eager_execution()\n",
    "tf.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69270138-96e8-45a3-b5b8-b364d352c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"Gender\"] = np.where(X_train[\"Gender\"] == \"Woman\", 1, 0).astype(np.int64)\n",
    "X_test[\"Gender\"] = np.where(X_test[\"Gender\"] == \"Woman\", 1, 0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "      (\"passthrough\", ['YearsCode', 'YearsCodePro', 'ComputerSkills', 'Gender']),\n",
    "      (StandardScaler(), ['PreviousSalary']),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "protected_adv = X_test[\"Gender\"].astype(str)\n",
    "privileged_adv = '0'\n",
    "\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "columns_names = preprocessor.get_feature_names_out(preprocessor.feature_names_in_)\n",
    "\n",
    "class ToFrame():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #print('ok')\n",
    "    \n",
    "    def fit(self, arr, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, arr, y=None):\n",
    "        df = pd.DataFrame(arr)\n",
    "        df.columns = columns_names\n",
    "        df.index = df['passthrough__Gender']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_model = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('toframe', ToFrame()),\n",
    "        ('adv', AdversarialDebiasing(prot_attr=['passthrough__Gender'], \n",
    "                                     debias=True,\n",
    "                                     verbose=False,\n",
    "                                     num_epochs=20,\n",
    "                                     adversary_loss_weight=1e-2,\n",
    "                                     random_state=42,\n",
    "                                     classifier_num_hidden_units=512))\n",
    "    ]\n",
    ")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241ca91",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32211a-cd35-4106-8899-5a3af55ad661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This cell may take some time to run!\n",
    "adv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af197697-d10a-49bf-86b1-8ba875156391",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b67bf0-db21-4790-a3d5-e7d01ca5832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_adv_model = dx.Explainer(adv_model, X_test, y_test, verbose=True)\n",
    "exp_adv_model.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49886297",
   "metadata": {},
   "source": [
    "#### Fairness performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bff184-55b7-4025-b6c1-e6e60623e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_adv_model = exp_adv_model.model_fairness(protected=protected_adv, privileged=privileged_adv, label=\"AdversarialTraining\")\n",
    "fairness_adv_model.fairness_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef280f-5dae-49db-8657-55eb1bf606e2",
   "metadata": {},
   "source": [
    "### Comparison to the decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb5e32-eda2-4c6f-a256-cc76bbed29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree_gender = exp_decisiontree.model_fairness(\n",
    "    protected=protected_adv, privileged=privileged_adv, label=\"DecisionTree\")\n",
    "fairness_decisiontree_gender.fairness_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2a80c-2ffd-461b-b04b-633179cca4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree_gender.plot([fairness_adv_model], type='radar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bedb0f",
   "metadata": {},
   "source": [
    "### Post-processing: ROC-pivot\n",
    "\n",
    "#### After-Training\n",
    "\n",
    "For this method, there is no re-training to do since it's a post-processing method. The idea is to alter results in favor / defavor of some groups to increase the fairness metrics scores (privileged group VS others).\n",
    "\n",
    "From a math point of view, \n",
    "\n",
    "Let, \n",
    "* `P` be the probability output of a model (higher probability means higher chances to get the favorable outcome, \"1\" in out case).\n",
    "* `cutoff` be the value to assign values to 0 (below cutoff) or 1 (above cutoff)\n",
    "* `𝜃` be the margin parameter to alter results (it is representing the notion of \"close enough\")\n",
    "* `Priviledge` be the boolean value if the observation is part of the priviledge group\n",
    "\n",
    "The roc pivot method will distinguish two cases : \n",
    "\n",
    "* The first one: if `|P - cutoff| < 𝜃 AND Priviledge AND P > cutoff` is `True` then the new probability became `P = cutoff - (P - cutoff)` which is now below the cutoff.\n",
    "\n",
    "* The second case: if `|P - cutoff| < 𝜃 AND NOT(Priviledge) AND cutoff > P` is `True`, then the new probability became `P = cutoff + (cutoff - P)` which is above the cutoff value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalex.fairness import roc_pivot\n",
    "exp_decisiontree_roc = copy(exp_decisiontree)\n",
    "\n",
    "# Results modifications. Theta arbitrarily set at 0.1\n",
    "exp_decisiontree_roc = roc_pivot(exp_decisiontree, protected, privileged, \n",
    "                                 theta = 0.1, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a78e6-71bb-4f4e-9d40-5f371fefad74",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b75a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decisiontree_roc.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b1f24-3b8d-4f96-a0eb-76bccb63fd29",
   "metadata": {},
   "source": [
    "#### Fairness performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444eeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree_roc = exp_decisiontree_roc.model_fairness(\n",
    "    protected, \n",
    "    privileged, \n",
    "    label='DecisionTreeClassifier_roc')\n",
    "\n",
    "fairness_decisiontree_roc.fairness_check(epsilon = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee689cd",
   "metadata": {},
   "source": [
    "Based on the fairness report, roc seems to have less effect on mitigating biais for this model.\n",
    "\n",
    "__Compare performance of the first model and the resampled one (visually)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1cecf-6897-44e5-8d93-6f68ebd3c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(\n",
    "    [fairness_decisiontree_roc, \n",
    "     fairness_decisiontree_resampled, \n",
    "     fairness_decisiontree_restr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(\n",
    "    [fairness_decisiontree_roc, \n",
    "     fairness_decisiontree_resampled, \n",
    "     fairness_decisiontree_restr], \n",
    "    type='radar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2786e64-c6b4-4813-a4ad-dad691b676d9",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "Now that you have seen several bias mitigation techniques, it is now your turn to play with the data. We list below several tasks that you may tackle, either on the dataset used so far or on another more involved one.\n",
    "\n",
    "### On this dataset\n",
    "\n",
    "* Change hyperparameters of the decision tree (typically, its depth) to optimize the accuracy and investigate whether there is a tradeoff between accuracy and fairness. You can also try other classifiers such as random forests.\n",
    "\n",
    "* Try other resampling or reweighting methods from Dalex to see if they perform better or worse for mitigating the biases. See the dalex [documentation](https://dalex.drwhy.ai/python/api/fairness/index.html) for help.\n",
    "\n",
    "\n",
    "### On another more complete dataset\n",
    "\n",
    "The dataframe `df_full` was also built from the Stackoverflow survey but contains more columns, specifically:\n",
    "- **MentalHealth**: whether the applicant has mental health issues *(categorical)*\n",
    "- **Accessibility**: whether the applicant has accessibility issues *(categorical)*\n",
    "- **Country**: country of origin of the applicant *(categorical)*\n",
    "- **HaveWorkedWith**: list of computer languages known by the applicant (note that the variable ComputerSkills is tthen he number of semicolon-separated skills) *(strings separated by semicolons)*\n",
    "\n",
    "This is now your turn to fully investigate this dataset: idenfity potential sources of biases, do some exploratory data analysis, build a classifier to predict the target variable Employed, check its biases and mitigate them with your favorite tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746daa3-b82e-4dc4-92da-a30986bf4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('stackoverflow_full.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8d122-40eb-4648-890b-755938107a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.sample(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81d088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
