{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7546d2ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "![title](images/datacraft-logo.png)\n",
    "\n",
    "<h1><center>AI Act Day</center></h1>\n",
    "<center><i>Datacraft workshop</i></center>\n",
    "\n",
    "\n",
    "**Objective of the notebook**: understand how to tackle the fairness challenge in AI on real use case - HR recrutment\n",
    "\n",
    "This exercise highly relies on the work conducted by the Dalex team (see [here](https://dalex.drwhy.ai/), [here](https://dalex.drwhy.ai/python-dalex-fairness.html) or [here](https://dalex.drwhy.ai/python-dalex-fairness2.html)). It is based on two [Stackoverflow survey](https://insights.stackoverflow.com/survey) (2021 & 2022) adapted to a recrutment issue.\n",
    "\n",
    "Our humble ambition is to combine the power of different tools: Dalex as a base, complemented by other tools from the open source community, like [AIF360](https://aif360.mybluemix.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3664e9",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prerequesites\" data-toc-modified-id=\"Prerequesites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prerequesites</a></span></li><li><span><a href=\"#Bias-a-priori\" data-toc-modified-id=\"Bias-a-priori-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bias a priori</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>EDA</a></span></li><li><span><a href=\"#ML-prerequisites\" data-toc-modified-id=\"ML-prerequisites-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>ML prerequisites</a></span></li><li><span><a href=\"#Fairness-evaluation-principles\" data-toc-modified-id=\"Fairness-evaluation-principles-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Fairness evaluation principles</a></span></li><li><span><a href=\"#Strategies\" data-toc-modified-id=\"Strategies-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Strategies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Do-nothing\" data-toc-modified-id=\"Do-nothing-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Do nothing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Algorithmic-performance\" data-toc-modified-id=\"Algorithmic-performance-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Algorithmic performance</a></span></li><li><span><a href=\"#Fairness-performance\" data-toc-modified-id=\"Fairness-performance-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>Fairness performance</a></span></li></ul></li><li><span><a href=\"#Remove-sensitive-attribute\" data-toc-modified-id=\"Remove-sensitive-attribute-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Remove sensitive attribute</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Algorithmic-performance\" data-toc-modified-id=\"Algorithmic-performance-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Algorithmic performance</a></span></li><li><span><a href=\"#Fairness-performance\" data-toc-modified-id=\"Fairness-performance-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>Fairness performance</a></span></li></ul></li><li><span><a href=\"#Adversarial-inprocessing\" data-toc-modified-id=\"Adversarial-inprocessing-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Adversarial inprocessing</a></span></li><li><span><a href=\"#Calibrate-equalized-ODTS\" data-toc-modified-id=\"Calibrate-equalized-ODTS-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Calibrate equalized ODTS</a></span></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Comparison</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82455bd4",
   "metadata": {},
   "source": [
    "## Prerequesites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165a31d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:53:50.393927Z",
     "start_time": "2021-11-08T16:53:50.384927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We strongly recommend to install the necessary libraries from a dedicated virtual environment\n",
    "# See Minicoda e.g.: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands\n",
    "\n",
    "# ! pip install fairlearn\n",
    "# ! pip install dalex -U\n",
    "# ! pip install -U scikit-learn\n",
    "# ! pip install -U pandas\n",
    "# ! pip install aif360\n",
    "# ! pip install -U plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecff085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:53:59.159825Z",
     "start_time": "2021-11-08T16:53:50.592927Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#import aif360\n",
    "import dalex as dx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from copy import copy\n",
    "#from aif360.datasets import BinaryLabelDataset\n",
    "#from aif360.metrics import ClassificationMetric\n",
    "\n",
    "{\n",
    "#    \"aif360\": aif360.__version__,\n",
    "    \"dalex\": dx.__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"sklearn\": sklearn.__version__\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65464feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ddb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:53:59.174788Z",
     "start_time": "2021-11-08T16:53:59.161789Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23066dcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Bias a priori\n",
    "\n",
    "*When implementing an AI system, fairness and biases must be an important component during conception, especially when dealing with sensitive information, and/or Personally Identifiable Information (PII), and/or Personal Health Information (PHI). Indeed, not only those information are bound to the law (GDPR in Europe e.g.), but they are also bound to a brand image challenge.*\n",
    "\n",
    "Today's example aims at **assigning a risk with recruitment data**.\n",
    "\n",
    "Before implementing any AI system to predict the likelihood of a candidate to be hired, **AI engineers AND business stakeholders** should:\n",
    "\n",
    "- Sit and identify potential sources of biases\n",
    "- Define a one or several metrics that will quantify the bias of the AI system\n",
    "\n",
    "![fairness_tree](images/fairness_tree.png)\n",
    "\n",
    "---\n",
    "\n",
    "In this case, potential biases might lie in:\n",
    "\n",
    "- Gender\n",
    "- Age\n",
    "- Revenue?\n",
    "- Technos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25809c3",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "TO DO: description of the columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2b9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:01.904204Z",
     "start_time": "2021-11-08T16:54:01.880199Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('stackoverflow_clean.csv', index_col=0)\n",
    "target = \"Employment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4e33f-71dc-45e6-bc9a-892c706b6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "df.sample(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4d631",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "The columns subject to fairness biases are Gender, MentalHealth, and Age. For now, we remove the variable MentalHealth but we will come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753fc04-e5a1-4b83-832e-6eded3c097b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_MentalHealth = df['MentalHealth']\n",
    "df = df.drop('MentalHealth', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby(\"Gender\")\n",
    "    [target]\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566dd5a-01bd-42d4-bb9f-c196538103f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby(\"Age\")\n",
    "    [target]\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f232136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:39:26.546261Z",
     "start_time": "2021-11-08T16:39:26.529227Z"
    },
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby(\"Gender\")\n",
    "    [target]\n",
    "    .value_counts(normalize=True)\n",
    "    .multiply(100)\n",
    "    .round(1)\n",
    "    .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6e2ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:39:26.546261Z",
     "start_time": "2021-11-08T16:39:26.529227Z"
    },
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby(\"Age\")\n",
    "    [target]\n",
    "    .value_counts(normalize=True)\n",
    "    .multiply(100)\n",
    "    .round(1)\n",
    "    .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e940a",
   "metadata": {},
   "source": [
    "### TODO : update here\n",
    "\n",
    "**Quick EDA**\n",
    "\n",
    "If we bin by gender, we realize that women are:\n",
    "- Under represented in volume (3778 vs 74853)     \n",
    "- Slightly biased from an employment perspective (41.3% vs 50.8%)\n",
    "\n",
    "If we bin by age, we realize that over 35 people are:\n",
    "- Under represented in volume (28737 vs 51456)\n",
    "- Slightly biased from an employment perspective (47.7% vs 751.9)\n",
    "\n",
    "\n",
    "If we combine the two, biases in input data are amplified.\n",
    "\n",
    "**Conclusion: the apriori from section 2 looks confirmed and will need to be carefully handled during modelling.**\n",
    "\n",
    "----\n",
    "\n",
    "**Notes**\n",
    "\n",
    "1. This is a toy example where biases are \"straightforward\" and well identified as \"recurrent\" social biases. However, it might not always be as easy to detect them. Additional sources might come from data history, selection bias, data incompleteness, unexpected sources of bias (column/attribute), ...\n",
    "2. In this example, we identified biases related to representation in volume. If we had not been exposed to such discrepencies, namely having a balanced dataset, could we have concluded that biases would have been limited while modelling? Not so sure, see [this article](https://arxiv.org/pdf/1811.08489.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327ea81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ML prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9f683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:11.096850Z",
     "start_time": "2021-11-08T16:54:11.078844Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=target),\n",
    "    df[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e90df",
   "metadata": {},
   "source": [
    "## Fairness evaluation principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6643757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T16:59:52.131656Z",
     "start_time": "2021-12-01T16:59:52.032435Z"
    }
   },
   "source": [
    "![dalex](images/dalex_pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77c1f3",
   "metadata": {},
   "source": [
    "The main object of dalex is the `Explainer` container which wraps a **dataset** (features and target) and a **trained model**. \n",
    "\n",
    "Once the data and the model have been wrapped, one needs to fix **protected and privileged attributes**.\n",
    "\n",
    "**Important note**: beware these choices correspond to an a priori understanding of the problem and could miss hidden flaws of the model. An interesting line of work would consist in conducting a kind of grid-search exploration for potential biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bbe92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:18.644263Z",
     "start_time": "2021-11-08T16:54:18.628267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Protected attribute is 0 if a man or non binary and 0 if a woman plus the age\n",
    "\n",
    "protected = (pd.Series(np.where(X_test[\"Gender\"] == \"Woman\", '1', '0'), index=X_test.index) \n",
    "             + '_' \n",
    "             + X_test.Age)\n",
    "protected_train = (pd.Series(np.where(X_train[\"Gender\"] == \"Woman\", '1', '0').astype(str), index=X_train.index) \n",
    "                   + '_' \n",
    "                   + X_train.Age)\n",
    "\n",
    "# Privileged population is men under 35 years old\n",
    "privileged = '0_<35'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736579d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Strategies\n",
    "\n",
    "Following section intends to implement different strategies to mitigate the bias:\n",
    "- No strategy implemented\n",
    "- Pre-processing strategy: edit the data priori to fitting a model\n",
    "- In-processing: change the way a model is trained, changing the loss function e.g.\n",
    "- Post-processing: edit the predictions once a model has been fitted\n",
    "- (Business-rule: implement a business that is intended to manually mitigate the bias, see this [article](https://towardsdatascience.com/tutorial-breaking-myths-about-ai-fairness-the-case-of-biased-automated-recruitment-9ee9b2ecc3a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d156f12",
   "metadata": {},
   "source": [
    "### Do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc2820",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b286cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:26.651675Z",
     "start_time": "2021-11-08T16:54:26.635675Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "      (\"passthrough\", make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\", sparse=False), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "clf_decisiontree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=10, random_state=123))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b937a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:27.294932Z",
     "start_time": "2021-11-08T16:54:27.250331Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf_decisiontree.fit(df.drop(columns=[target]), df[target])\n",
    "clf_decisiontree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8889fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:54:56.205145Z",
     "start_time": "2021-11-08T16:54:56.177145Z"
    }
   },
   "outputs": [],
   "source": [
    "# exp_decisiontree = dx.Explainer(clf_decisiontree, df.drop(columns=[target]), df[target], verbose=False)\n",
    "exp_decisiontree = dx.Explainer(clf_decisiontree, X_test, y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259302a4",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33dff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:04.966640Z",
     "start_time": "2021-11-08T16:55:04.910641Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_decisiontree.model_performance().result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ee622-7631-4a3a-8799-034819b9d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(30, 30))\n",
    "# tree.plot_tree(clf_decisiontree['classifier'], \n",
    "#                feature_names=clf_decisiontree['preprocessor'].get_feature_names(),\n",
    "#               class_names= ['employed', 'unemployed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f2be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fairness performance\n",
    "\n",
    "Quoting Dalex' tutorial:\n",
    "\n",
    "\n",
    "> The idea is that ratios between scores of privileged and unprivileged metrics should be close to 1. The closer, the fairer. To relax this criterion a little bit, it can be written more thoughtfully:\n",
    "\n",
    "> $$ \\forall i \\in \\{a, b, ..., z\\}, \\quad \\epsilon < \\frac{metric_i}{metric_{privileged}} < \\frac{1}{\\epsilon}.$$\n",
    "\n",
    "> Where the epsilon is a value between 0 and 1, it should be a minimum acceptable value of the ratio. On default, it is 0.8, which adheres to four-fifths rule (80% rule) often looked at in hiring, for example.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aec8ac",
   "metadata": {},
   "source": [
    "The method `model_fairness` returns a fairness object from which fairness evaluations can be conducted. Notice that every metrics inherited from the confusion matrix are computed during the instantiation.\n",
    "\n",
    "Two methods can then be performed:\n",
    "- The `fairness_check` method, which returns a report on the fairness of the model. It requires an epsilon parameter that corresponds to the threshold ratio below which a given metric is considered to be unfair (default value is 0.8).\n",
    "- The `plot` method, which allows to visualize the main fairness ratios between the protected subgroups and the privileged one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18a463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:10.877525Z",
     "start_time": "2021-11-08T16:55:10.854526Z"
    }
   },
   "outputs": [],
   "source": [
    "fairness_decisiontree = exp_decisiontree.model_fairness(protected=protected, privileged=privileged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83c47a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:11.815330Z",
     "start_time": "2021-11-08T16:55:11.795325Z"
    }
   },
   "outputs": [],
   "source": [
    "fairness_decisiontree.fairness_check(epsilon = 0.8) # default epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caca6b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T16:55:15.708181Z",
     "start_time": "2021-11-08T16:55:13.726179Z"
    }
   },
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc40d8f",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "1. Fairness metrics work the exact same way as performance metrics do. If one was to fit a model on the entire dataset and foster overfitting (namely, skipping a `train_test_split` operation), she would end up with a non biased model.\n",
    "2. A lots of metrics can be computed. It is important to define early in the conception which are the critical metrics to monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc19112",
   "metadata": {},
   "source": [
    "### Pre-processing: Remove sensitive attribute\n",
    "\n",
    "The first thing that can come to mind is to remove sensitive variables. However, this option is considered as really naive since it can have no effect. \n",
    "\n",
    "**When may this method works ?**\n",
    "\n",
    "➤ If the sensitive variable conveys the bias (mostly) on its own.\n",
    "\n",
    "    ➤ This means, the sensitive variable is correlated with the target variable\n",
    "    ➤ This also means, the sensitive variable is NOT correlated at all with other explanatory variables and any combination of other explanatory variables cannot be used as a proxi for the sensitive variable. \n",
    "\n",
    "Most of the time, the bias is shared accross explanatory variables. For example, a bias on gender may be present in many ones (salary, education, socio-professional category, etc.)\n",
    "\n",
    "This transformation is not possible in the dalex module (since it not a recommended option to deal with bias). To implement it, a new model has to be trained and explained. This time the sensitive variable\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain a model without sensitive variables \"age\" and \"sex\"\n",
    "X_train_restricted = X_train.drop(['Gender', 'Age'], axis=1)\n",
    "\n",
    "clf_decisiontree_restr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "])\n",
    "\n",
    "clf_decisiontree_restr.fit(X_train_restricted, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215a2d9",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601caa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dalex explainer for the model without sensitive variables\n",
    "exp_decisiontree_restr = dx.Explainer(clf_decisiontree_restr, X_test, y_test, verbose=True)\n",
    "\n",
    "exp_decisiontree_restr.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34616f",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "1. Here are the performance metrics for the new model. Results are quite similar to those of the model with all variables. However, considering we want to unbias results, it's quite difficult to use these metrics to compare models. Indeed, having a 100% precision on predicting a bias decision is far from our goal event if the model is perfect (at its predicting job).\n",
    "\n",
    "#### Fairness performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54298d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see effect of removing sensitive variables on bias metrics\n",
    "fairness_decisiontree_restr = exp_decisiontree_restr.model_fairness(protected=protected, privileged=privileged, \n",
    "                                                                    label='DecisionTreeClassifier_no_sensitive')\n",
    "\n",
    "fairness_decisiontree_restr.fairness_check(epsilon = 0.8) # default epsilon\n",
    "\n",
    "fairness_decisiontree_restr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57210c",
   "metadata": {},
   "source": [
    "Without a real surprise, the effect of removing sensitive variables did not unbias our results (according to 2 metrics used as reference. Even if that did not entirely solved the ou r bias issues, we can admit it seems less bias than before. In dalex it's also possible to compare models performances (according to bias metrics). Below, there is the comparison between the decision tree with and without sensitive variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2 (or more) fairness objects in dalex (add them as list in parameters). \n",
    "# It's also possible to choose the plot type ! \n",
    "fairness_decisiontree_restr.plot([fairness_decisiontree], type='radar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7295d5c",
   "metadata": {},
   "source": [
    "Now, let's check more appropriate ways to remove unfairness bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655120e8",
   "metadata": {},
   "source": [
    "### Pre-processing: Re-sampling\n",
    "\n",
    "Dalex provide 2 types of resampling methods and 1 reweighting method. In this tutorial only the basic resampling is showed.\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalex.fairness import resample\n",
    "clf_resampled = copy(clf_decisiontree) # Create a copy to not alter the main object\n",
    "\n",
    "# Resampling observations\n",
    "indices_uniform = resample(protected_train, y_train, verbose = False)\n",
    "\n",
    "# Re-fit model with resampled data\n",
    "clf_resampled.fit(X_train.reset_index(drop=True).iloc[indices_uniform, :], y_train.reset_index(drop=True)[indices_uniform])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59593803",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decisiontree_resampled = dx.Explainer(clf_resampled, X_test, y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d3d3d",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3094f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decisiontree_resampled.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75950a",
   "metadata": {},
   "source": [
    "#### Fairness performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree_resampled = exp_decisiontree_resampled.model_fairness(\n",
    "    protected, privileged, label='DecisionTreeClassifier_resampled')\n",
    "\n",
    "fairness_decisiontree_resampled.fairness_check(epsilon = 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e469789",
   "metadata": {},
   "source": [
    "__Compare performance of the first model and the resampled one (visually)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot([fairness_decisiontree_resampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9be4ee",
   "metadata": {},
   "source": [
    "The resampling method is parlty random but it should increase the fairness of outputs at least on few fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f237c-705d-4695-a498-2548f6e46fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot([fairness_decisiontree_resampled, fairness_decisiontree_restr], type='radar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288399a3-1bdf-4282-a662-0fcfd11fb013",
   "metadata": {},
   "source": [
    "### TO DO (Optional): add a section on explainability of decision trees, look at feature importance to understand where the bias comes from and how the model works -> explainable IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4074164",
   "metadata": {},
   "source": [
    "### In-processing: Adversarial training\n",
    "\n",
    "The adversarial inprocessing method consists in learning a target attribute $y$ (here the risk) while forgetting a fixed sensitive attribute $s$. This is done by learning a neural network and minimizing a loss of the form:\n",
    "$$ \\mathcal{L} = \\mathcal{L}_{CE}(y,\\hat{y}) - \\lambda \\mathcal{L}_{CE}(s,\\hat{s}), $$\n",
    "where $\\lambda$ controls the fairness-accuracy tradeoff. \n",
    "\n",
    "This method is implemented in aif360 in the case of a binary sensitive attribute. In the following we incorporate it into the dalex pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.sklearn.inprocessing import AdversarialDebiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "sess = tf.Session()\n",
    "tf.disable_eager_execution()\n",
    "tf.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in, target_in = df, \"Employment\"\n",
    "\n",
    "df_in[\"Gender\"] = np.where(df_in[\"Gender\"] == \"Woman\", 1, 0).astype(np.int64)\n",
    "\n",
    "X_train_in, X_test_in, y_train_in, y_test_in = train_test_split(\n",
    "    df_in.drop(columns=[target_in]),\n",
    "    df_in[target_in],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "      (\"passthrough\", make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "X_prep_train =  preprocessor.fit_transform(X_train_in)\n",
    "columns_names = preprocessor.get_feature_names_out(preprocessor.feature_names_in_)\n",
    "\n",
    "protected = np.where(X_test[\"Gender\"] == \"Woman\", 1, 0).astype(str)\n",
    "privileged = '0'\n",
    "\n",
    "\n",
    "class ToFrame():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #print('ok')\n",
    "    \n",
    "    def fit(self, arr, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, arr, y=None):\n",
    "        df = pd.DataFrame(arr)\n",
    "        df.columns = columns_names\n",
    "        df.index = df['passthrough__Gender']\n",
    "        #print(df.head())\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for adv_weight in ['no_adv', 'adv']:\n",
    "    if adv_weight == 'no_adv':\n",
    "        models.append(\n",
    "            \n",
    "                        Pipeline(steps=[\n",
    "                            ('preprocessor', preprocessor),\n",
    "                            ('toframe', ToFrame()),\n",
    "                            ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "                        ])       \n",
    "                     )\n",
    "    else:\n",
    "        models.append(\n",
    "                    Pipeline(steps=[\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('toframe', ToFrame()),\n",
    "                        ('adv', AdversarialDebiasing(prot_attr='passthrough__Gender', \n",
    "                                                     debias=True\n",
    "                                                    ))\n",
    "                            ])        \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1945c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_expl = []\n",
    "for model in models:\n",
    "    model.fit(X_train_in, y_train_in)\n",
    "    models_expl.append(\n",
    "                        dx.Explainer(model, X_test_in, y_test_in, verbose=False)\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fobj = []\n",
    "for i, expl in enumerate(models_expl):\n",
    "    fobj.append(\n",
    "                expl.model_fairness(protected=protected, privileged=privileged)\n",
    "    )\n",
    "    fobj[-1].label ='adv_{}'.format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "fobj[0].fairness_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15207331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fobj[1].fairness_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fobj[0].plot(objects=[fobj[1]], type = \"radar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bedb0f",
   "metadata": {},
   "source": [
    "### Post-processing: ROC-pivot\n",
    "\n",
    "#### After-Training\n",
    "\n",
    "For this method, there is no re-training to do since it's a post-processing method. The idea is to alter results in favor / defavor of some groups to increase the fairness metrics scores (privileged group VS others).\n",
    "\n",
    "From a math point of view, \n",
    "\n",
    "Let, \n",
    "* `P` be the probability output of a model (higher probability means higher chances to get the favorable outcome, \"1\" in out case).\n",
    "* `cutoff` be the value to assign values to 0 (below cutoff) or 1 (above cutoff)\n",
    "* `𝜃` be the margin parameter to alter results (it is representing the notion of \"close enough\")\n",
    "* `Priviledge` be the boolean value if the observation is part of the priviledge group\n",
    "\n",
    "The roc pivot method will distinguish two cases : \n",
    "\n",
    "* The first one: if `|P - cutoff| < 𝜃 AND Priviledge AND P > cutoff` is `True` then the new probability became `P = cutoff - (P - cutoff)` which is now below the cutoff.\n",
    "\n",
    "* The second case: if `|P - cutoff| < 𝜃 AND NOT(Priviledge) AND cutoff > P` is `True`, then the new probability became `P = cutoff + (cutoff - P)` which is above the cutoff value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalex.fairness import roc_pivot\n",
    "exp_decisiontree_roc = copy(exp_decisiontree)\n",
    "\n",
    "# Results modifications. Theta arbitrarily set at 0.1\n",
    "exp_decisiontree_roc = roc_pivot(exp_decisiontree, protected, privileged, \n",
    "                                 theta = 0.1, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241ca91",
   "metadata": {},
   "source": [
    "#### Algorithmic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b75a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decisiontree_roc.model_performance().result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49886297",
   "metadata": {},
   "source": [
    "#### Fairness performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444eeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree_roc = exp_decisiontree_roc.model_fairness(\n",
    "    protected, \n",
    "    privileged, \n",
    "    label='DecisionTreeClassifier_roc')\n",
    "\n",
    "fairness_decisiontree_roc.fairness_check(epsilon = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee689cd",
   "metadata": {},
   "source": [
    "Based on the fairness report, roc seems to have no effect on mitigating biais for this model.\n",
    "\n",
    "__Compare performance of the first model and the resampled one (visually)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1cecf-6897-44e5-8d93-6f68ebd3c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(\n",
    "    [fairness_decisiontree_roc, \n",
    "     fairness_decisiontree_resampled, \n",
    "     fairness_decisiontree_restr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(\n",
    "    [fairness_decisiontree_roc, \n",
    "     fairness_decisiontree_resampled, \n",
    "     fairness_decisiontree_restr], \n",
    "    type='radar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb87f5",
   "metadata": {},
   "source": [
    "## TO DO: update here\n",
    "\n",
    "At this point it is possible to evaluate the impact of this method (with this treshold). Based on the default fairness metrics this methode seems to provide:\n",
    "\n",
    "* A quite better fairness for the 1st, 3rd and 5th (even if not perfect)\n",
    "* A mitigate result for the 2rd and 4th ones\n",
    "\n",
    "According to the fairness metric needed for a given project, the roc method seems to be a quite efficient solution to reduce some bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41590a36",
   "metadata": {},
   "source": [
    "### Calibrate equalized ODTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915439f",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07445142",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(objects=[fairness_sensitive, fairness_adversarial, fairness_odts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(objects=[fairness_sensitive, fairness_adversarial, fairness_odts], type = \"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(objects=[fairness_sensitive, fairness_adversarial, fairness_odts], type = \"stacked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(objects=[fairness_sensitive, fairness_adversarial, fairness_odts], type = \"performance_and_fairness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18518728",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_decisiontree.plot(objects=[fairness_sensitive, fairness_adversarial, fairness_odts], type = \"ceteris_paribus_cutoff\", subgroup=\"female_young\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
